# Jupyter Notebook Examples

This repository contains a collection of Jupyter Notebook examples showcasing different functionalities and use cases.

## Notebook Files

1. [LLaVa_NeXt_Video_demo_inference.ipynb](./LLaVa_NeXt_Video_demo_inference.ipynb)
   - Description: Demonstrates inference using the LLaVa NeXt Video model.

2. [Llava_demo_4bit.ipynb](./Llava_demo_4bit.ipynb)
   - Description: Demonstrates a 4-bit LLaVa model.

3. [gpt-4-image-text-speech-Yosemite.ipynb](./gpt-4-image-text-speech-Yosemite.ipynb)
   - Description: Combines GPT-4 with image, text, and speech capabilities using the Yosemite dataset.

4. [gpt-4-image-text-speech.ipynb](./gpt-4-image-text-speech.ipynb)
   - Description: Showcases GPT-4 with image, text, and speech integration.

5. [gpt_4_image_text_speech_example_1.ipynb](./gpt_4_image_text_speech_example_1.ipynb)
   - Description: Example notebook demonstrating GPT-4 with image, text, and speech.

6. [gpt_4_image_text_speech_mount_rushmor...](./gpt_4_image_text_speech_mount_rushmor...)
   - Description: GPT-4 example using image, text, and speech with the Mount Rushmore dataset.

7. [llava-2-Mistral-7B.ipynb](./llava-2-Mistral-7B.ipynb)
   - Description: Utilizes the Mistral-7B model with LLaVa-2.
8. [llava-fine-tune-with-custom-data](./Fine_tune_LLaVa_on_a_custom_dataset.ipynb)
   - Description: LLaVA fine tuning with receipt datasets and output in JSON
## Getting Started

To run the notebook files locally, follow these steps:

1. Clone the repository:
   ```
   git clone https://github.com/your-username/your-repo.git
   ```

2. Install the required dependencies. It is recommended to use a virtual environment:
   ```
   cd your-repo
   python -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

3. Launch Jupyter Notebook:
   ```
   jupyter notebook
   ```

4. Open the desired notebook file from the list above and run the cells to execute the code.

## Contributing

Contributions to this repository are welcome. If you have any new notebook examples or improvements to existing ones, please feel free to open a pull request.

## License

This project is licensed under the [MIT License](LICENSE).
